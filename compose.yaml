version: '3.8'

services:
  backend:
    build:
      context: .
      dockerfile: infrastructure/docker/backend.Dockerfile
    container_name: wizqueue-backend
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - PORT=3000
      - DATABASE_HOST=${DATABASE_HOST}
      - DATABASE_PORT=${DATABASE_PORT}
      - DATABASE_NAME=${DATABASE_NAME}
      - DATABASE_USER=${DATABASE_USER}
      - DATABASE_PASSWORD=${DATABASE_PASSWORD}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-minicpm-v:8b}
      - UPLOAD_DIR=/app/uploads
      - MAX_FILE_SIZE=${MAX_FILE_SIZE:-10485760}
    volumes:
      - uploads:/app/uploads
    networks:
      - wizqueue
    depends_on:
      - ollama
    extra_hosts:
      - "host.docker.internal:host-gateway"

  frontend:
    build:
      context: .
      dockerfile: infrastructure/docker/frontend.Dockerfile
      args:
        - VITE_API_URL=${VITE_API_URL:-http://localhost:3000/api}
    container_name: wizqueue-frontend
    restart: unless-stopped
    ports:
      - "80:80"
    networks:
      - wizqueue
    depends_on:
      - backend

  ollama:
    image: ollama/ollama:latest
    container_name: wizqueue-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    networks:
      - wizqueue
    # Uncomment if you have GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  uploads:
    driver: local
  ollama:
    driver: local

networks:
  wizqueue:
    driver: bridge
